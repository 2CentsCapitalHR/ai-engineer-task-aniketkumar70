{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# INSTALLATION AND SETUP\n",
        "# ============================================================================\n",
        "\n",
        "# Install required packages\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_packages():\n",
        "    \"\"\"Install all required packages\"\"\"\n",
        "    packages = [\n",
        "        'gradio>=4.46.0',\n",
        "        'python-docx',\n",
        "        'PyPDF2',\n",
        "        'unidecode',\n",
        "        'openai',\n",
        "        'sentence-transformers',\n",
        "        'faiss-cpu',\n",
        "        'pandas',\n",
        "        'numpy',\n",
        "        'transformers',\n",
        "        'torch',\n",
        "        'datasets'\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", package],\n",
        "                         check=True, capture_output=True)\n",
        "            print(f\"✓ Installed {package}\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"✗ Failed to install {package}: {e}\")\n",
        "\n",
        "# Uncomment the line below to install packages\n",
        "# install_packages()\n",
        "\n",
        "# ============================================================================\n",
        "# IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import json\n",
        "import tempfile\n",
        "import shutil\n",
        "import warnings\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Document processing\n",
        "from docx import Document\n",
        "from docx.enum.text import WD_COLOR_INDEX\n",
        "from docx.shared import Pt, RGBColor\n",
        "from docx.oxml.shared import qn\n",
        "from PyPDF2 import PdfReader\n",
        "from unidecode import unidecode\n",
        "\n",
        "# ML and NLP\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "import faiss\n",
        "\n",
        "# UI\n",
        "import gradio as gr\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION AND CONSTANTS\n",
        "# ============================================================================\n",
        "\n",
        "# ADGM reference file paths (update these paths according to your setup)\n",
        "REFERENCE_FILES = {\n",
        "    'checklist_1': '/content/ADGM CHECKLIST 2.pdf',\n",
        "    'checklist_2': '/content/ADGM checklist.pdf',\n",
        "    'employment_2019': '/content/ADGM Standard Employment Contract - ER 2019 - Short Version (May 2024).docx',\n",
        "    'employment_2024': '/content/ADGM Standard Employment Contract Template - ER 2024 (Feb 2025).docx',\n",
        "    'data_sources': '/content/Data Sources.docx',\n",
        "    'data_protection': '/content/OFFICE OF DATA PROTECTION.pdf',\n",
        "    'resolution_template': '/content/adgm-ra-resolution-multiple-incorporate-shareholders-LTD-incorporation-v2.docx'\n",
        "}\n",
        "\n",
        "# Enhanced field mapping for employment contracts\n",
        "ENHANCED_FIELD_MAP = {\n",
        "    \"Employer name\": {\n",
        "        \"variations\": [\"employer\", \"company name\", \"entity name\", \"organization\"],\n",
        "        \"required\": True,\n",
        "        \"adgm_ref\": \"ER 2024, Section 3.1\"\n",
        "    },\n",
        "    \"Employee name\": {\n",
        "        \"variations\": [\"employee name\", \"individual\", \"worker\", \"staff member\"],\n",
        "        \"required\": True,\n",
        "        \"adgm_ref\": \"ER 2024, Section 3.2\"\n",
        "    },\n",
        "    \"Date of commencement\": {\n",
        "        \"variations\": [\"start date\", \"joining date\", \"commencement date\", \"employment start\"],\n",
        "        \"required\": True,\n",
        "        \"adgm_ref\": \"ER 2024, Section 4.1\"\n",
        "    },\n",
        "    \"Job title\": {\n",
        "        \"variations\": [\"job title\", \"designation\", \"position\", \"role\"],\n",
        "        \"required\": True,\n",
        "        \"adgm_ref\": \"ER 2024, Section 4.2\"\n",
        "    },\n",
        "    \"Wages\": {\n",
        "        \"variations\": [\"salary\", \"remuneration\", \"compensation\", \"wages\", \"pay\"],\n",
        "        \"required\": True,\n",
        "        \"adgm_ref\": \"ER 2024, Section 5.1\"\n",
        "    },\n",
        "    \"Pay period\": {\n",
        "        \"variations\": [\"pay period\", \"monthly\", \"payment frequency\", \"salary period\"],\n",
        "        \"required\": True,\n",
        "        \"adgm_ref\": \"ER 2024, Section 5.2\"\n",
        "    },\n",
        "    \"Hours or days of work\": {\n",
        "        \"variations\": [\"working hours\", \"work time\", \"hours of work\", \"working days\"],\n",
        "        \"required\": True,\n",
        "        \"adgm_ref\": \"ER 2024, Section 6.1\"\n",
        "    },\n",
        "    \"Vacation leave\": {\n",
        "        \"variations\": [\"annual leave\", \"holiday entitlement\", \"vacation\", \"leave\"],\n",
        "        \"required\": True,\n",
        "        \"adgm_ref\": \"ER 2024, Section 7.1\"\n",
        "    },\n",
        "    \"Sick leave\": {\n",
        "        \"variations\": [\"sick leave\", \"medical leave\", \"illness leave\"],\n",
        "        \"required\": True,\n",
        "        \"adgm_ref\": \"ER 2024, Section 7.2\"\n",
        "    },\n",
        "    \"Notice period\": {\n",
        "        \"variations\": [\"notice period\", \"termination notice\", \"resignation notice\"],\n",
        "        \"required\": True,\n",
        "        \"adgm_ref\": \"ER 2024, Section 8.1\"\n",
        "    },\n",
        "    \"Term (if fixed-term)\": {\n",
        "        \"variations\": [\"contract term\", \"duration\", \"fixed term\", \"contract period\"],\n",
        "        \"required\": False,\n",
        "        \"adgm_ref\": \"ER 2024, Section 4.3\"\n",
        "    },\n",
        "    \"Place of work or Remote employee\": {\n",
        "        \"variations\": [\"place of work\", \"remote employee\", \"work location\", \"workplace\"],\n",
        "        \"required\": True,\n",
        "        \"adgm_ref\": \"ER 2024, Section 4.4\"\n",
        "    },\n",
        "    \"Disciplinary/Grievance reference\": {\n",
        "        \"variations\": [\"grievance procedure\", \"disciplinary\", \"disciplinary procedure\"],\n",
        "        \"required\": True,\n",
        "        \"adgm_ref\": \"ER 2024, Section 9.1\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Document type detection patterns\n",
        "DOCUMENT_TYPES = {\n",
        "    \"employment_contract\": {\n",
        "        \"keywords\": [\"employment contract\", \"er 2019\", \"er 2024\", \"employment regulations\",\n",
        "                    \"terms of employment\", \"contract of employment\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"apd\": {\n",
        "        \"keywords\": [\"appropriate policy document\", \"apd\", \"data protection regulations\",\n",
        "                    \"gdpr\", \"data protection policy\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"resolution\": {\n",
        "        \"keywords\": [\"resolution\", \"authorised signatory\", \"adoption of articles\",\n",
        "                    \"board resolution\", \"shareholder resolution\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"articles_of_association\": {\n",
        "        \"keywords\": [\"articles of association\", \"aoa\", \"company articles\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"memorandum_of_association\": {\n",
        "        \"keywords\": [\"memorandum of association\", \"moa\", \"company memorandum\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"branch_registration\": {\n",
        "        \"keywords\": [\"branch registration\", \"branch in adgm\", \"branch application\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"business_plan\": {\n",
        "        \"keywords\": [\"business plan\", \"financial projections\", \"business model\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"incorporation_form\": {\n",
        "        \"keywords\": [\"incorporation application\", \"company registration form\",\n",
        "                    \"incorporation form\"],\n",
        "        \"weight\": 1.0\n",
        "    },\n",
        "    \"ubo_declaration\": {\n",
        "        \"keywords\": [\"ubo declaration\", \"ultimate beneficial owner\", \"beneficial ownership\"],\n",
        "        \"weight\": 1.0\n",
        "    }\n",
        "}\n",
        "\n",
        "# Process checklists based on ADGM requirements\n",
        "PROCESS_CHECKLISTS = {\n",
        "    \"Company Incorporation\": {\n",
        "        \"documents\": [\n",
        "            \"Business plan\",\n",
        "            \"Articles of Association\",\n",
        "            \"Memorandum of Association\",\n",
        "            \"Resolution authorising incorporation\",\n",
        "            \"Incorporation application form\",\n",
        "            \"Authorised Signatories (appointment evidence)\",\n",
        "            \"Directors (details/evidence)\",\n",
        "            \"Registered office (lease + lease registration)\",\n",
        "            \"Shareholders details\",\n",
        "            \"Ultimate Beneficial Owners (UBO Declaration)\",\n",
        "            \"Data Protection contact\"\n",
        "        ],\n",
        "        \"description\": \"Complete company incorporation in ADGM\"\n",
        "    },\n",
        "    \"Branch Registration\": {\n",
        "        \"documents\": [\n",
        "            \"Parent Articles of Association (certified, recent)\",\n",
        "            \"Latest audited financials of parent company\",\n",
        "            \"Parent board resolution to register branch\",\n",
        "            \"Registered office in ADGM (lease + registration)\",\n",
        "            \"Authorised signatories (UAE/GCC/resident requirement)\",\n",
        "            \"All parent directors and secretaries details\",\n",
        "            \"Shareholders of parent company (details)\",\n",
        "            \"Ultimate Beneficial Owners of parent (25%+ ownership)\",\n",
        "            \"Data Protection contact\"\n",
        "        ],\n",
        "        \"description\": \"Registration of foreign company branch in ADGM\"\n",
        "    },\n",
        "    \"Employment/HR\": {\n",
        "        \"documents\": [\n",
        "            \"Employment contract (ER 2024 compliant)\",\n",
        "            \"HR policy reference (disciplinary/grievance procedures)\",\n",
        "            \"Evidence of ADGM entity (for jurisdiction verification)\"\n",
        "        ],\n",
        "        \"description\": \"Employment documentation compliance\"\n",
        "    },\n",
        "    \"Data Protection\": {\n",
        "        \"documents\": [\n",
        "            \"Appropriate Policy Document (APD)\",\n",
        "            \"Record of Processing Activities (RoPA) reference\",\n",
        "            \"Privacy Notice (internal/external) references\",\n",
        "            \"Data Protection Impact Assessment (if required)\"\n",
        "        ],\n",
        "        \"description\": \"Data protection compliance documentation\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# ADGM-specific red flags and compliance checks\n",
        "ADGM_RED_FLAGS = {\n",
        "    \"jurisdiction\": {\n",
        "        \"correct\": [\"abu dhabi global market\", \"adgm\", \"adgm courts\", \"adgm jurisdiction\"],\n",
        "        \"incorrect\": [\"uae federal courts\", \"dubai courts\", \"abu dhabi courts\", \"sharjah courts\"],\n",
        "        \"severity\": \"High\",\n",
        "        \"reference\": \"ADGM Companies Regulations 2020, Article 6\"\n",
        "    },\n",
        "    \"governing_law\": {\n",
        "        \"required\": [\"adgm common law\", \"english common law\", \"adgm laws\"],\n",
        "        \"severity\": \"High\",\n",
        "        \"reference\": \"ADGM Courts Law 2013\"\n",
        "    },\n",
        "    \"registered_office\": {\n",
        "        \"required\": [\"registered office\", \"adgm address\", \"al maryah island\"],\n",
        "        \"severity\": \"Medium\",\n",
        "        \"reference\": \"ADGM Companies Regulations 2020, Article 15\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# RAG SYSTEM IMPLEMENTATION\n",
        "# ============================================================================\n",
        "\n",
        "class ADGMKnowledgeBase:\n",
        "    \"\"\"RAG system for ADGM legal knowledge\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.embeddings_model = None\n",
        "        self.knowledge_base = []\n",
        "        self.embeddings = None\n",
        "        self.index = None\n",
        "        self.initialize_model()\n",
        "\n",
        "    def initialize_model(self):\n",
        "        \"\"\"Initialize the sentence transformer model\"\"\"\n",
        "        try:\n",
        "            self.embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "            print(\"✓ RAG embeddings model initialized\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Failed to initialize embeddings model: {e}\")\n",
        "            # Fallback to simpler keyword matching\n",
        "            self.embeddings_model = None\n",
        "\n",
        "    def load_reference_documents(self):\n",
        "        \"\"\"Load and process ADGM reference documents\"\"\"\n",
        "        print(\"Loading ADGM reference documents...\")\n",
        "\n",
        "        for ref_name, file_path in REFERENCE_FILES.items():\n",
        "            try:\n",
        "                if os.path.exists(file_path):\n",
        "                    content = self._extract_text_from_file(file_path)\n",
        "                    if content:\n",
        "                        # Split content into chunks\n",
        "                        chunks = self._split_into_chunks(content, ref_name)\n",
        "                        self.knowledge_base.extend(chunks)\n",
        "                        print(f\"✓ Loaded {ref_name}: {len(chunks)} chunks\")\n",
        "                else:\n",
        "                    print(f\"⚠ Reference file not found: {file_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Error loading {ref_name}: {e}\")\n",
        "\n",
        "        if self.knowledge_base and self.embeddings_model:\n",
        "            self._create_embeddings_index()\n",
        "\n",
        "        print(f\"Knowledge base loaded with {len(self.knowledge_base)} chunks\")\n",
        "\n",
        "    def _extract_text_from_file(self, file_path: str) -> str:\n",
        "        \"\"\"Extract text from PDF or DOCX files\"\"\"\n",
        "        try:\n",
        "            if file_path.lower().endswith('.pdf'):\n",
        "                reader = PdfReader(file_path)\n",
        "                return \"\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
        "            elif file_path.lower().endswith('.docx'):\n",
        "                doc = Document(file_path)\n",
        "                texts = [para.text for para in doc.paragraphs if para.text.strip()]\n",
        "                # Also extract table content\n",
        "                for table in doc.tables:\n",
        "                    for row in table.rows:\n",
        "                        texts.append(\" | \".join([cell.text for cell in row.cells]))\n",
        "                return \"\\n\".join(texts)\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text from {file_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "    def _split_into_chunks(self, content: str, source: str) -> List[Dict]:\n",
        "        \"\"\"Split content into manageable chunks\"\"\"\n",
        "        # Split by sentences/paragraphs\n",
        "        sentences = re.split(r'[.!?]\\s+', content)\n",
        "        chunks = []\n",
        "\n",
        "        current_chunk = \"\"\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if not sentence:\n",
        "                continue\n",
        "\n",
        "            # If adding this sentence would make chunk too long, start new chunk\n",
        "            if len(current_chunk) + len(sentence) > 500:\n",
        "                if current_chunk:\n",
        "                    chunks.append({\n",
        "                        'content': current_chunk.strip(),\n",
        "                        'source': source,\n",
        "                        'length': len(current_chunk)\n",
        "                    })\n",
        "                current_chunk = sentence\n",
        "            else:\n",
        "                current_chunk += (\" \" if current_chunk else \"\") + sentence\n",
        "\n",
        "        # Add final chunk\n",
        "        if current_chunk.strip():\n",
        "            chunks.append({\n",
        "                'content': current_chunk.strip(),\n",
        "                'source': source,\n",
        "                'length': len(current_chunk)\n",
        "            })\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _create_embeddings_index(self):\n",
        "        \"\"\"Create FAISS index for similarity search\"\"\"\n",
        "        if not self.embeddings_model:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            texts = [chunk['content'] for chunk in self.knowledge_base]\n",
        "            self.embeddings = self.embeddings_model.encode(texts)\n",
        "\n",
        "            # Create FAISS index\n",
        "            dimension = self.embeddings.shape[1]\n",
        "            self.index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
        "\n",
        "            # Normalize embeddings for cosine similarity\n",
        "            faiss.normalize_L2(self.embeddings)\n",
        "            self.index.add(self.embeddings)\n",
        "\n",
        "            print(f\"✓ Created FAISS index with {len(texts)} embeddings\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error creating embeddings index: {e}\")\n",
        "\n",
        "    def search_relevant_content(self, query: str, top_k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Search for relevant content using RAG\"\"\"\n",
        "        if not self.embeddings_model or not self.index:\n",
        "            # Fallback to keyword search\n",
        "            return self._keyword_search(query, top_k)\n",
        "\n",
        "        try:\n",
        "            # Encode query\n",
        "            query_embedding = self.embeddings_model.encode([query])\n",
        "            faiss.normalize_L2(query_embedding)\n",
        "\n",
        "            # Search\n",
        "            scores, indices = self.index.search(query_embedding, top_k)\n",
        "\n",
        "            results = []\n",
        "            for score, idx in zip(scores[0], indices[0]):\n",
        "                if idx < len(self.knowledge_base):\n",
        "                    chunk = self.knowledge_base[idx].copy()\n",
        "                    chunk['relevance_score'] = float(score)\n",
        "                    results.append(chunk)\n",
        "\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            print(f\"Error in RAG search: {e}\")\n",
        "            return self._keyword_search(query, top_k)\n",
        "\n",
        "    def _keyword_search(self, query: str, top_k: int) -> List[Dict]:\n",
        "        \"\"\"Fallback keyword-based search\"\"\"\n",
        "        query_words = normalize_text(query).split()\n",
        "        results = []\n",
        "\n",
        "        for chunk in self.knowledge_base:\n",
        "            content_normalized = normalize_text(chunk['content'])\n",
        "            score = sum(1 for word in query_words if word in content_normalized)\n",
        "\n",
        "            if score > 0:\n",
        "                chunk_copy = chunk.copy()\n",
        "                chunk_copy['relevance_score'] = score / len(query_words)\n",
        "                results.append(chunk_copy)\n",
        "\n",
        "        # Sort by relevance and return top_k\n",
        "        results.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
        "        return results[:top_k]\n",
        "\n",
        "# ============================================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    \"\"\"Normalize text for comparison\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    return unidecode(re.sub(r'\\s+', ' ', text.strip())).lower()\n",
        "\n",
        "def read_docx_text(file_path: str) -> str:\n",
        "    \"\"\"Extract text from DOCX file\"\"\"\n",
        "    try:\n",
        "        doc = Document(file_path)\n",
        "        texts = []\n",
        "\n",
        "        # Extract paragraph text\n",
        "        for para in doc.paragraphs:\n",
        "            if para.text.strip():\n",
        "                texts.append(para.text)\n",
        "\n",
        "        # Extract table text\n",
        "        for table in doc.tables:\n",
        "            for row in table.rows:\n",
        "                row_text = \" | \".join([cell.text for cell in row.cells])\n",
        "                if row_text.strip():\n",
        "                    texts.append(row_text)\n",
        "\n",
        "        return \"\\n\".join(texts)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading DOCX file {file_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def read_pdf_text(file_path: str) -> str:\n",
        "    \"\"\"Extract text from PDF file\"\"\"\n",
        "    try:\n",
        "        reader = PdfReader(file_path)\n",
        "        texts = []\n",
        "        for page in reader.pages:\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                texts.append(text)\n",
        "        return \"\\n\".join(texts)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF file {file_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def detect_document_type(text: str, filename: str = \"\") -> str:\n",
        "    \"\"\"Detect document type using enhanced pattern matching\"\"\"\n",
        "    normalized_text = normalize_text(text)\n",
        "    normalized_filename = normalize_text(filename)\n",
        "\n",
        "    best_type = \"unknown\"\n",
        "    best_score = 0\n",
        "\n",
        "    for doc_type, config in DOCUMENT_TYPES.items():\n",
        "        score = 0\n",
        "\n",
        "        # Check keywords in content\n",
        "        for keyword in config[\"keywords\"]:\n",
        "            if normalize_text(keyword) in normalized_text:\n",
        "                score += config[\"weight\"]\n",
        "\n",
        "        # Check keywords in filename\n",
        "        for keyword in config[\"keywords\"]:\n",
        "            if normalize_text(keyword) in normalized_filename:\n",
        "                score += config[\"weight\"] * 0.5  # Lower weight for filename matches\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_type = doc_type\n",
        "\n",
        "    return best_type if best_score > 0 else \"unknown\"\n",
        "\n",
        "# ============================================================================\n",
        "# DOCUMENT ANALYSIS FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def analyze_employment_contract(text: str, knowledge_base: ADGMKnowledgeBase) -> Dict:\n",
        "    \"\"\"Comprehensive analysis of employment contracts\"\"\"\n",
        "    normalized_text = normalize_text(text)\n",
        "\n",
        "    result = {\n",
        "        \"missing_fields\": [],\n",
        "        \"found_fields\": [],\n",
        "        \"issues\": [],\n",
        "        \"compliance_score\": 0,\n",
        "        \"adgm_references\": []\n",
        "    }\n",
        "\n",
        "    # Check required fields\n",
        "    for field_name, config in ENHANCED_FIELD_MAP.items():\n",
        "        field_found = False\n",
        "\n",
        "        for variation in config[\"variations\"]:\n",
        "            if normalize_text(variation) in normalized_text:\n",
        "                result[\"found_fields\"].append(field_name)\n",
        "                field_found = True\n",
        "                break\n",
        "\n",
        "        if not field_found and config[\"required\"]:\n",
        "            result[\"missing_fields\"].append(field_name)\n",
        "            result[\"issues\"].append({\n",
        "                \"issue\": f\"Missing required field: {field_name}\",\n",
        "                \"severity\": \"High\",\n",
        "                \"suggestion\": f\"Add {field_name} as required by {config['adgm_ref']}\",\n",
        "                \"adgm_reference\": config['adgm_ref']\n",
        "            })\n",
        "\n",
        "    # Check ADGM jurisdiction compliance\n",
        "    adgm_jurisdiction_found = False\n",
        "    for jurisdiction_term in ADGM_RED_FLAGS[\"jurisdiction\"][\"correct\"]:\n",
        "        if normalize_text(jurisdiction_term) in normalized_text:\n",
        "            adgm_jurisdiction_found = True\n",
        "            break\n",
        "\n",
        "    if not adgm_jurisdiction_found:\n",
        "        # Check for incorrect jurisdictions\n",
        "        incorrect_jurisdiction = None\n",
        "        for incorrect_term in ADGM_RED_FLAGS[\"jurisdiction\"][\"incorrect\"]:\n",
        "            if normalize_text(incorrect_term) in normalized_text:\n",
        "                incorrect_jurisdiction = incorrect_term\n",
        "                break\n",
        "\n",
        "        issue_text = \"No ADGM jurisdiction reference found\"\n",
        "        if incorrect_jurisdiction:\n",
        "            issue_text = f\"Incorrect jurisdiction reference found: {incorrect_jurisdiction}\"\n",
        "\n",
        "        result[\"issues\"].append({\n",
        "            \"issue\": issue_text,\n",
        "            \"severity\": \"High\",\n",
        "            \"suggestion\": \"Add governing law clause referencing ADGM Courts and ADGM jurisdiction\",\n",
        "            \"adgm_reference\": ADGM_RED_FLAGS[\"jurisdiction\"][\"reference\"]\n",
        "        })\n",
        "\n",
        "    # Check governing law\n",
        "    governing_law_found = False\n",
        "    for law_term in ADGM_RED_FLAGS[\"governing_law\"][\"required\"]:\n",
        "        if normalize_text(law_term) in normalized_text:\n",
        "            governing_law_found = True\n",
        "            break\n",
        "\n",
        "    if not governing_law_found:\n",
        "        result[\"issues\"].append({\n",
        "            \"issue\": \"No explicit governing law clause found\",\n",
        "            \"severity\": \"Medium\",\n",
        "            \"suggestion\": \"Add clause specifying ADGM Common Law as governing law\",\n",
        "            \"adgm_reference\": ADGM_RED_FLAGS[\"governing_law\"][\"reference\"]\n",
        "        })\n",
        "\n",
        "    # Use RAG to find relevant ADGM employment regulations\n",
        "    if knowledge_base and knowledge_base.embeddings_model:\n",
        "        employment_query = \"employment contract requirements ADGM ER 2024\"\n",
        "        relevant_content = knowledge_base.search_relevant_content(employment_query, top_k=3)\n",
        "\n",
        "        for content in relevant_content:\n",
        "            if content['relevance_score'] > 0.3:  # Threshold for relevance\n",
        "                result[\"adgm_references\"].append({\n",
        "                    \"source\": content[\"source\"],\n",
        "                    \"content_preview\": content[\"content\"][:200] + \"...\",\n",
        "                    \"relevance\": content[\"relevance_score\"]\n",
        "                })\n",
        "\n",
        "    # Calculate compliance score\n",
        "    total_required = sum(1 for config in ENHANCED_FIELD_MAP.values() if config[\"required\"])\n",
        "    found_required = len([f for f in result[\"found_fields\"]\n",
        "                         if ENHANCED_FIELD_MAP[f][\"required\"]])\n",
        "\n",
        "    field_score = (found_required / total_required) * 70 if total_required > 0 else 0\n",
        "    jurisdiction_score = 20 if adgm_jurisdiction_found else 0\n",
        "    governing_law_score = 10 if governing_law_found else 0\n",
        "\n",
        "    result[\"compliance_score\"] = field_score + jurisdiction_score + governing_law_score\n",
        "\n",
        "    return result\n",
        "\n",
        "def analyze_corporate_document(text: str, doc_type: str, knowledge_base: ADGMKnowledgeBase) -> Dict:\n",
        "    \"\"\"Analyze corporate documents (Articles, Resolutions, etc.)\"\"\"\n",
        "    normalized_text = normalize_text(text)\n",
        "\n",
        "    result = {\n",
        "        \"issues\": [],\n",
        "        \"compliance_score\": 0,\n",
        "        \"adgm_references\": []\n",
        "    }\n",
        "\n",
        "    # Common checks for corporate documents\n",
        "    # Check ADGM jurisdiction\n",
        "    if not any(normalize_text(term) in normalized_text\n",
        "               for term in ADGM_RED_FLAGS[\"jurisdiction\"][\"correct\"]):\n",
        "        result[\"issues\"].append({\n",
        "            \"issue\": \"ADGM jurisdiction not specified\",\n",
        "            \"severity\": \"High\",\n",
        "            \"suggestion\": \"Specify ADGM Courts jurisdiction\",\n",
        "            \"adgm_reference\": ADGM_RED_FLAGS[\"jurisdiction\"][\"reference\"]\n",
        "        })\n",
        "\n",
        "    # Check registered office requirement\n",
        "    if not any(normalize_text(term) in normalized_text\n",
        "               for term in ADGM_RED_FLAGS[\"registered_office\"][\"required\"]):\n",
        "        result[\"issues\"].append({\n",
        "            \"issue\": \"Registered office address not clearly specified\",\n",
        "            \"severity\": \"Medium\",\n",
        "            \"suggestion\": \"Include proper ADGM registered office address\",\n",
        "            \"adgm_reference\": ADGM_RED_FLAGS[\"registered_office\"][\"reference\"]\n",
        "        })\n",
        "\n",
        "    # Document-specific checks\n",
        "    if doc_type == \"articles_of_association\":\n",
        "        result.update(analyze_articles_of_association(normalized_text))\n",
        "    elif doc_type == \"resolution\":\n",
        "        result.update(analyze_resolution(normalized_text))\n",
        "    elif doc_type == \"memorandum_of_association\":\n",
        "        result.update(analyze_memorandum(normalized_text))\n",
        "\n",
        "    # Use RAG for document-specific guidance\n",
        "    if knowledge_base and knowledge_base.embeddings_model:\n",
        "        query = f\"{doc_type} requirements ADGM incorporation\"\n",
        "        relevant_content = knowledge_base.search_relevant_content(query, top_k=3)\n",
        "\n",
        "        for content in relevant_content:\n",
        "            if content['relevance_score'] > 0.3:\n",
        "                result[\"adgm_references\"].append({\n",
        "                    \"source\": content[\"source\"],\n",
        "                    \"content_preview\": content[\"content\"][:200] + \"...\",\n",
        "                    \"relevance\": content[\"relevance_score\"]\n",
        "                })\n",
        "\n",
        "    # Calculate basic compliance score\n",
        "    high_issues = len([i for i in result[\"issues\"] if i[\"severity\"] == \"High\"])\n",
        "    medium_issues = len([i for i in result[\"issues\"] if i[\"severity\"] == \"Medium\"])\n",
        "\n",
        "    result[\"compliance_score\"] = max(0, 100 - (high_issues * 25) - (medium_issues * 10))\n",
        "\n",
        "    return result\n",
        "\n",
        "def analyze_articles_of_association(normalized_text: str) -> Dict:\n",
        "    \"\"\"Specific analysis for Articles of Association\"\"\"\n",
        "    issues = []\n",
        "\n",
        "    # Check for essential clauses\n",
        "    essential_clauses = {\n",
        "        \"company name\": [\"company name\", \"name of the company\"],\n",
        "        \"registered office\": [\"registered office\", \"registered address\"],\n",
        "        \"objects\": [\"objects\", \"purposes\", \"business activities\"],\n",
        "        \"share capital\": [\"share capital\", \"authorized capital\", \"capital\"],\n",
        "        \"directors\": [\"directors\", \"board of directors\"],\n",
        "        \"shareholders\": [\"shareholders\", \"members\"]\n",
        "    }\n",
        "\n",
        "    for clause_name, keywords in essential_clauses.items():\n",
        "        if not any(keyword in normalized_text for keyword in keywords):\n",
        "            issues.append({\n",
        "                \"issue\": f\"Missing or unclear {clause_name} clause\",\n",
        "                \"severity\": \"High\",\n",
        "                \"suggestion\": f\"Include proper {clause_name} clause as per ADGM requirements\",\n",
        "                \"adgm_reference\": \"ADGM Companies Regulations 2020\"\n",
        "            })\n",
        "\n",
        "    return {\"issues\": issues}\n",
        "\n",
        "def analyze_resolution(normalized_text: str) -> Dict:\n",
        "    \"\"\"Specific analysis for Resolutions\"\"\"\n",
        "    issues = []\n",
        "\n",
        "    # Check for resolution essentials\n",
        "    resolution_essentials = {\n",
        "        \"date\": [\"date\", \"dated\"],\n",
        "        \"quorum\": [\"quorum\", \"present\"],\n",
        "        \"authorization\": [\"authorize\", \"authorise\", \"resolved\"],\n",
        "        \"signature\": [\"signature\", \"signed\", \"director\"]\n",
        "    }\n",
        "\n",
        "    for essential, keywords in resolution_essentials.items():\n",
        "        if not any(keyword in normalized_text for keyword in keywords):\n",
        "            issues.append({\n",
        "                \"issue\": f\"Missing {essential} in resolution\",\n",
        "                \"severity\": \"Medium\",\n",
        "                \"suggestion\": f\"Include proper {essential} as required for valid resolutions\",\n",
        "                \"adgm_reference\": \"ADGM Companies Regulations 2020\"\n",
        "            })\n",
        "\n",
        "    return {\"issues\": issues}\n",
        "\n",
        "def analyze_memorandum(normalized_text: str) -> Dict:\n",
        "    \"\"\"Specific analysis for Memorandum of Association\"\"\"\n",
        "    issues = []\n",
        "\n",
        "    # Check for memorandum essentials\n",
        "    memo_essentials = {\n",
        "        \"subscribers\": [\"subscribers\", \"founding members\"],\n",
        "        \"incorporation\": [\"incorporation\", \"form a company\"],\n",
        "        \"liability\": [\"liability\", \"limited liability\"],\n",
        "        \"capital\": [\"capital\", \"share capital\"]\n",
        "    }\n",
        "\n",
        "    for essential, keywords in memo_essentials.items():\n",
        "        if not any(keyword in normalized_text for keyword in keywords):\n",
        "            issues.append({\n",
        "                \"issue\": f\"Missing {essential} clause in memorandum\",\n",
        "                \"severity\": \"High\",\n",
        "                \"suggestion\": f\"Include {essential} clause as required by ADGM\",\n",
        "                \"adgm_reference\": \"ADGM Companies Regulations 2020\"\n",
        "            })\n",
        "\n",
        "    return {\"issues\": issues}\n",
        "\n",
        "# ============================================================================\n",
        "# PROCESS DETECTION AND CHECKLIST VERIFICATION\n",
        "# ============================================================================\n",
        "\n",
        "def guess_legal_process(document_types: List[str], filenames: List[str]) -> str:\n",
        "    \"\"\"Intelligently guess the legal process from uploaded documents\"\"\"\n",
        "\n",
        "    # Count document type occurrences\n",
        "    type_counts = {}\n",
        "    for doc_type in document_types:\n",
        "        type_counts[doc_type] = type_counts.get(doc_type, 0) + 1\n",
        "\n",
        "    # Analyze filenames for additional context\n",
        "    filename_text = \" \".join(filenames).lower()\n",
        "\n",
        "    # Decision logic\n",
        "    if any(dt in [\"articles_of_association\", \"memorandum_of_association\", \"resolution\"]\n",
        "           for dt in document_types):\n",
        "        if \"branch\" in filename_text or any(\"branch\" in dt for dt in document_types):\n",
        "            return \"Branch Registration\"\n",
        "        else:\n",
        "            return \"Company Incorporation\"\n",
        "\n",
        "    elif any(dt == \"employment_contract\" for dt in document_types):\n",
        "        return \"Employment/HR\"\n",
        "\n",
        "    elif any(dt == \"apd\" for dt in document_types):\n",
        "        return \"Data Protection\"\n",
        "\n",
        "    elif \"branch\" in filename_text:\n",
        "        return \"Branch Registration\"\n",
        "\n",
        "    elif any(keyword in filename_text for keyword in [\"incorporation\", \"company\", \"articles\"]):\n",
        "        return \"Company Incorporation\"\n",
        "\n",
        "    # Default to most common process\n",
        "    return \"Company Incorporation\"\n",
        "\n",
        "def check_process_completeness(process: str, uploaded_filenames: List[str]) -> Tuple[int, int, List[str]]:\n",
        "    \"\"\"Check completeness of uploaded documents against process requirements\"\"\"\n",
        "\n",
        "    if process not in PROCESS_CHECKLISTS:\n",
        "        return 0, 0, []\n",
        "\n",
        "    required_docs = PROCESS_CHECKLISTS[process][\"documents\"]\n",
        "    uploaded_normalized = [normalize_text(name) for name in uploaded_filenames]\n",
        "\n",
        "    present_docs = []\n",
        "    missing_docs = []\n",
        "\n",
        "    for required_doc in required_docs:\n",
        "        doc_found = False\n",
        "        required_normalized = normalize_text(required_doc)\n",
        "        required_keywords = required_normalized.split()\n",
        "\n",
        "        # Check if any uploaded file matches this requirement\n",
        "        for uploaded_name in uploaded_normalized:\n",
        "            # Simple keyword matching - if 2+ keywords match, consider it present\n",
        "            matches = sum(1 for keyword in required_keywords if keyword in uploaded_name)\n",
        "\n",
        "            if matches >= min(2, len(required_keywords)):\n",
        "                present_docs.append(required_doc)\n",
        "                doc_found = True\n",
        "                break\n",
        "\n",
        "        if not doc_found:\n",
        "            missing_docs.append(required_doc)\n",
        "\n",
        "    return len(present_docs), len(required_docs), missing_docs\n",
        "\n",
        "# ============================================================================\n",
        "# DOCUMENT ANNOTATION AND OUTPUT GENERATION\n",
        "# ============================================================================\n",
        "\n",
        "def annotate_docx_document(input_path: str, issues: List[Dict], output_path: str = None) -> bytes:\n",
        "    \"\"\"Add review comments to DOCX document\"\"\"\n",
        "    try:\n",
        "        doc = Document(input_path)\n",
        "\n",
        "        # Add a review summary at the beginning\n",
        "        if issues:\n",
        "            summary_para = doc.paragraphs[0].insert_paragraph_before()\n",
        "            summary_run = summary_para.add_run(\n",
        "                f\"\\n=== ADGM COMPLIANCE REVIEW SUMMARY ===\\n\"\n",
        "                f\"Total Issues Found: {len(issues)}\\n\"\n",
        "                f\"Review Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
        "                f\"{'='*50}\\n\"\n",
        "            )\n",
        "            summary_run.font.size = Pt(10)\n",
        "            summary_run.font.color.rgb = RGBColor(255, 0, 0)  # Red color\n",
        "            summary_run.bold = True\n",
        "\n",
        "        # Process each issue\n",
        "        for i, issue in enumerate(issues):\n",
        "            issue_inserted = False\n",
        "\n",
        "            # Try to find relevant paragraph to insert comment\n",
        "            for para in doc.paragraphs:\n",
        "                para_text_normalized = normalize_text(para.text)\n",
        "\n",
        "                # Look for keywords from the issue in the paragraph\n",
        "                issue_keywords = normalize_text(issue.get(\"issue\", \"\")).split()[:3]  # First 3 words\n",
        "\n",
        "                if any(keyword in para_text_normalized for keyword in issue_keywords if keyword):\n",
        "                    # Insert comment after this paragraph\n",
        "                    comment_para = para.insert_paragraph_after()\n",
        "\n",
        "                    # Create formatted comment\n",
        "                    comment_text = (\n",
        "                        f\"[REVIEW #{i+1}] \"\n",
        "                        f\"Issue: {issue.get('issue', 'Unknown issue')}\\n\"\n",
        "                        f\"Severity: {issue.get('severity', 'Medium')}\\n\"\n",
        "                        f\"Suggestion: {issue.get('suggestion', 'Please review')}\\n\"\n",
        "                        f\"ADGM Reference: {issue.get('adgm_reference', 'General ADGM Regulations')}\\n\"\n",
        "                    )\n",
        "\n",
        "                    comment_run = comment_para.add_run(comment_text)\n",
        "                    comment_run.font.size = Pt(9)\n",
        "\n",
        "                    # Color code by severity\n",
        "                    if issue.get('severity') == 'High':\n",
        "                        comment_run.font.color.rgb = RGBColor(255, 0, 0)  # Red\n",
        "                    elif issue.get('severity') == 'Medium':\n",
        "                        comment_run.font.color.rgb = RGBColor(255, 165, 0)  # Orange\n",
        "                    else:\n",
        "                        comment_run.font.color.rgb = RGBColor(0, 0, 255)  # Blue\n",
        "\n",
        "                    comment_run.italic = True\n",
        "                    issue_inserted = True\n",
        "                    break\n",
        "\n",
        "            # If issue wasn't inserted, add it at the end\n",
        "            if not issue_inserted:\n",
        "                end_para = doc.add_paragraph()\n",
        "                comment_text = (\n",
        "                    f\"\\n[REVIEW #{i+1}] \"\n",
        "                    f\"Issue: {issue.get('issue', 'Unknown issue')}\\n\"\n",
        "                    f\"Severity: {issue.get('severity', 'Medium')}\\n\"\n",
        "                    f\"Suggestion: {issue.get('suggestion', 'Please review')}\\n\"\n",
        "                    f\"ADGM Reference: {issue.get('adgm_reference', 'General ADGM Regulations')}\\n\"\n",
        "                )\n",
        "\n",
        "                comment_run = end_para.add_run(comment_text)\n",
        "                comment_run.font.size = Pt(9)\n",
        "\n",
        "                if issue.get('severity') == 'High':\n",
        "                    comment_run.font.color.rgb = RGBColor(255, 0, 0)\n",
        "                elif issue.get('severity') == 'Medium':\n",
        "                    comment_run.font.color.rgb = RGBColor(255, 165, 0)\n",
        "                else:\n",
        "                    comment_run.font.color.rgb = RGBColor(0, 0, 255)\n",
        "\n",
        "                comment_run.italic = True\n",
        "\n",
        "        # Save to BytesIO\n",
        "        output_buffer = io.BytesIO()\n",
        "        doc.save(output_buffer)\n",
        "        output_buffer.seek(0)\n",
        "\n",
        "        # If output path provided, also save to file\n",
        "        if output_path:\n",
        "            doc.save(output_path)\n",
        "\n",
        "        return output_buffer.getvalue()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error annotating document {input_path}: {e}\")\n",
        "        return b\"\"\n",
        "\n",
        "def generate_compliance_report(analysis_results: Dict) -> Dict:\n",
        "    \"\"\"Generate comprehensive compliance report\"\"\"\n",
        "\n",
        "    # Calculate overall compliance metrics\n",
        "    total_issues = len(analysis_results.get(\"issues_found\", []))\n",
        "    high_severity_issues = len([i for i in analysis_results.get(\"issues_found\", [])\n",
        "                               if i.get(\"severity\") == \"High\"])\n",
        "    medium_severity_issues = len([i for i in analysis_results.get(\"issues_found\", [])\n",
        "                                 if i.get(\"severity\") == \"Medium\"])\n",
        "\n",
        "    # Calculate overall compliance score\n",
        "    if total_issues == 0:\n",
        "        overall_compliance = 100\n",
        "    else:\n",
        "        penalty = (high_severity_issues * 25) + (medium_severity_issues * 10)\n",
        "        overall_compliance = max(0, 100 - penalty)\n",
        "\n",
        "    # Determine compliance status\n",
        "    if overall_compliance >= 90:\n",
        "        compliance_status = \"Excellent\"\n",
        "    elif overall_compliance >= 75:\n",
        "        compliance_status = \"Good\"\n",
        "    elif overall_compliance >= 50:\n",
        "        compliance_status = \"Needs Improvement\"\n",
        "    else:\n",
        "        compliance_status = \"Poor - Significant Issues\"\n",
        "\n",
        "    # Generate recommendations\n",
        "    recommendations = []\n",
        "    if high_severity_issues > 0:\n",
        "        recommendations.append(\"Address all high-severity issues immediately before submission\")\n",
        "    if medium_severity_issues > 0:\n",
        "        recommendations.append(\"Review and resolve medium-severity issues for better compliance\")\n",
        "    if analysis_results.get(\"missing_document\"):\n",
        "        recommendations.append(\"Prepare and upload all missing required documents\")\n",
        "\n",
        "    if not recommendations:\n",
        "        recommendations.append(\"Documents appear to be in good compliance with ADGM requirements\")\n",
        "\n",
        "    # Prepare final report\n",
        "    report = {\n",
        "        \"analysis_summary\": {\n",
        "            \"process_type\": analysis_results.get(\"process\", \"Unknown\"),\n",
        "            \"documents_uploaded\": analysis_results.get(\"documents_uploaded\", 0),\n",
        "            \"required_documents\": analysis_results.get(\"required_documents\", 0),\n",
        "            \"completeness_percentage\": round(\n",
        "                (analysis_results.get(\"documents_uploaded\", 0) /\n",
        "                 max(1, analysis_results.get(\"required_documents\", 1))) * 100, 1\n",
        "            ),\n",
        "            \"missing_documents\": analysis_results.get(\"missing_document\", []),\n",
        "            \"overall_compliance_score\": overall_compliance,\n",
        "            \"compliance_status\": compliance_status\n",
        "        },\n",
        "        \"issue_breakdown\": {\n",
        "            \"total_issues\": total_issues,\n",
        "            \"high_severity\": high_severity_issues,\n",
        "            \"medium_severity\": medium_severity_issues,\n",
        "            \"low_severity\": total_issues - high_severity_issues - medium_severity_issues\n",
        "        },\n",
        "        \"detailed_issues\": analysis_results.get(\"issues_found\", []),\n",
        "        \"recommendations\": recommendations,\n",
        "        \"next_steps\": [\n",
        "            \"Review all flagged issues in the annotated documents\",\n",
        "            \"Consult ADGM regulations for specific requirements\",\n",
        "            \"Consider professional legal review before final submission\",\n",
        "            \"Prepare any missing documents identified in the analysis\"\n",
        "        ],\n",
        "        \"adgm_references\": list(set([\n",
        "            issue.get(\"adgm_reference\", \"\")\n",
        "            for issue in analysis_results.get(\"issues_found\", [])\n",
        "            if issue.get(\"adgm_reference\")\n",
        "        ])),\n",
        "        \"generated_on\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S UTC\"),\n",
        "        \"disclaimer\": \"This automated review is for guidance only. Professional legal advice should be sought for final compliance verification.\"\n",
        "    }\n",
        "\n",
        "    return report\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN ANALYSIS ENGINE\n",
        "# ============================================================================\n",
        "\n",
        "def analyze_uploaded_documents(file_paths: List[str], knowledge_base: ADGMKnowledgeBase) -> Tuple[List[str], str]:\n",
        "    \"\"\"Main document analysis function\"\"\"\n",
        "\n",
        "    print(f\"Starting analysis of {len(file_paths)} documents...\")\n",
        "\n",
        "    # Initialize results\n",
        "    results = {\n",
        "        \"process\": None,\n",
        "        \"documents_uploaded\": 0,\n",
        "        \"required_documents\": 0,\n",
        "        \"missing_document\": [],\n",
        "        \"issues_found\": [],\n",
        "        \"document_analysis\": []\n",
        "    }\n",
        "\n",
        "    annotated_paths = []\n",
        "    document_types = []\n",
        "    filenames = []\n",
        "\n",
        "    # Process each uploaded file\n",
        "    for file_path in file_paths:\n",
        "        try:\n",
        "            filename = os.path.basename(file_path)\n",
        "            filenames.append(filename)\n",
        "\n",
        "            print(f\"Processing: {filename}\")\n",
        "\n",
        "            # Extract text based on file type\n",
        "            if file_path.lower().endswith('.docx'):\n",
        "                document_text = read_docx_text(file_path)\n",
        "                can_annotate = True\n",
        "            elif file_path.lower().endswith('.pdf'):\n",
        "                document_text = read_pdf_text(file_path)\n",
        "                can_annotate = False\n",
        "            else:\n",
        "                print(f\"Unsupported file type: {filename}\")\n",
        "                continue\n",
        "\n",
        "            if not document_text.strip():\n",
        "                print(f\"No text extracted from: {filename}\")\n",
        "                continue\n",
        "\n",
        "            # Detect document type\n",
        "            doc_type = detect_document_type(document_text, filename)\n",
        "            document_types.append(doc_type)\n",
        "\n",
        "            print(f\"Detected type: {doc_type}\")\n",
        "\n",
        "            # Analyze document based on type\n",
        "            doc_issues = []\n",
        "            doc_analysis = {\n",
        "                \"filename\": filename,\n",
        "                \"document_type\": doc_type,\n",
        "                \"issues\": [],\n",
        "                \"compliance_score\": 0\n",
        "            }\n",
        "\n",
        "            if doc_type == \"employment_contract\":\n",
        "                analysis_result = analyze_employment_contract(document_text, knowledge_base)\n",
        "                doc_issues.extend(analysis_result.get(\"issues\", []))\n",
        "                doc_analysis.update({\n",
        "                    \"missing_fields\": analysis_result.get(\"missing_fields\", []),\n",
        "                    \"found_fields\": analysis_result.get(\"found_fields\", []),\n",
        "                    \"compliance_score\": analysis_result.get(\"compliance_score\", 0)\n",
        "                })\n",
        "\n",
        "            elif doc_type in [\"articles_of_association\", \"memorandum_of_association\", \"resolution\"]:\n",
        "                analysis_result = analyze_corporate_document(document_text, doc_type, knowledge_base)\n",
        "                doc_issues.extend(analysis_result.get(\"issues\", []))\n",
        "                doc_analysis[\"compliance_score\"] = analysis_result.get(\"compliance_score\", 0)\n",
        "\n",
        "            else:\n",
        "                # Generic document analysis\n",
        "                analysis_result = analyze_corporate_document(document_text, doc_type, knowledge_base)\n",
        "                doc_issues.extend(analysis_result.get(\"issues\", []))\n",
        "                doc_analysis[\"compliance_score\"] = analysis_result.get(\"compliance_score\", 0)\n",
        "\n",
        "            # Add document reference to issues\n",
        "            for issue in doc_issues:\n",
        "                issue[\"document\"] = filename\n",
        "                issue[\"document_type\"] = doc_type\n",
        "\n",
        "            doc_analysis[\"issues\"] = doc_issues\n",
        "            results[\"document_analysis\"].append(doc_analysis)\n",
        "            results[\"issues_found\"].extend(doc_issues)\n",
        "\n",
        "            # Annotate DOCX files if there are issues\n",
        "            if can_annotate and doc_issues:\n",
        "                try:\n",
        "                    reviewed_path = file_path.replace('.docx', '_REVIEWED.docx')\n",
        "                    annotate_docx_document(file_path, doc_issues, reviewed_path)\n",
        "                    annotated_paths.append(reviewed_path)\n",
        "                    print(f\"✓ Annotated: {filename}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"✗ Failed to annotate {filename}: {e}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Determine legal process and check completeness\n",
        "    if document_types:\n",
        "        legal_process = guess_legal_process(document_types, filenames)\n",
        "        present_count, required_count, missing_docs = check_process_completeness(legal_process, filenames)\n",
        "\n",
        "        results.update({\n",
        "            \"process\": legal_process,\n",
        "            \"documents_uploaded\": present_count,\n",
        "            \"required_documents\": required_count,\n",
        "            \"missing_document\": missing_docs\n",
        "        })\n",
        "\n",
        "        print(f\"Detected process: {legal_process}\")\n",
        "        print(f\"Document completeness: {present_count}/{required_count}\")\n",
        "\n",
        "    # Generate comprehensive report\n",
        "    comprehensive_report = generate_compliance_report(results)\n",
        "\n",
        "    # Format final output\n",
        "    final_output = {\n",
        "        \"basic_results\": results,\n",
        "        \"comprehensive_report\": comprehensive_report,\n",
        "        \"annotated_files\": [os.path.basename(path) for path in annotated_paths]\n",
        "    }\n",
        "\n",
        "    return annotated_paths, json.dumps(final_output, indent=2, ensure_ascii=False)\n",
        "\n",
        "# ============================================================================\n",
        "# GRADIO INTERFACE\n",
        "# ============================================================================\n",
        "\n",
        "def create_gradio_interface():\n",
        "    \"\"\"Create the Gradio interface for the ADGM Corporate Agent\"\"\"\n",
        "\n",
        "    # Initialize knowledge base\n",
        "    print(\"Initializing ADGM Knowledge Base...\")\n",
        "    kb = ADGMKnowledgeBase()\n",
        "    kb.load_reference_documents()\n",
        "\n",
        "    def process_files_interface(files):\n",
        "        \"\"\"Interface function for Gradio\"\"\"\n",
        "        if not files:\n",
        "            return \"❌ No files uploaded\", \"Please upload at least one document for analysis.\"\n",
        "\n",
        "        try:\n",
        "            # Create temporary directory\n",
        "            temp_dir = tempfile.mkdtemp(prefix=\"adgm_analysis_\")\n",
        "            file_paths = []\n",
        "\n",
        "            # Copy uploaded files to temp directory\n",
        "            for file_obj in files:\n",
        "                temp_path = os.path.join(temp_dir, os.path.basename(file_obj.name))\n",
        "                shutil.copy2(file_obj.name, temp_path)\n",
        "                file_paths.append(temp_path)\n",
        "\n",
        "            # Perform analysis\n",
        "            annotated_files, report_json = analyze_uploaded_documents(file_paths, kb)\n",
        "\n",
        "            # Parse the report for summary\n",
        "            try:\n",
        "                report_data = json.loads(report_json)\n",
        "                comprehensive_report = report_data.get(\"comprehensive_report\", {})\n",
        "                analysis_summary = comprehensive_report.get(\"analysis_summary\", {})\n",
        "\n",
        "                # Create status summary\n",
        "                status_summary = f\"\"\"\n",
        "✅ **Analysis Complete**\n",
        "\n",
        "📊 **Process Detected**: {analysis_summary.get('process_type', 'Unknown')}\n",
        "\n",
        "📄 **Document Status**: {analysis_summary.get('documents_uploaded', 0)}/{analysis_summary.get('required_documents', 0)} required documents uploaded ({analysis_summary.get('completeness_percentage', 0)}% complete)\n",
        "\n",
        "🎯 **Compliance Score**: {analysis_summary.get('overall_compliance_score', 0)}/100 ({analysis_summary.get('compliance_status', 'Unknown')})\n",
        "\n",
        "⚠️ **Issues Found**: {comprehensive_report.get('issue_breakdown', {}).get('total_issues', 0)} total\n",
        "   - High Severity: {comprehensive_report.get('issue_breakdown', {}).get('high_severity', 0)}\n",
        "   - Medium Severity: {comprehensive_report.get('issue_breakdown', {}).get('medium_severity', 0)}\n",
        "   - Low Severity: {comprehensive_report.get('issue_breakdown', {}).get('low_severity', 0)}\n",
        "\n",
        "📝 **Reviewed Files**: {len(annotated_files)} document(s) annotated with review comments\n",
        "\n",
        "🔍 **Missing Documents**: {len(analysis_summary.get('missing_documents', []))} required document(s) missing\n",
        "\"\"\"\n",
        "\n",
        "                # Add missing documents list if any\n",
        "                if analysis_summary.get('missing_documents'):\n",
        "                    status_summary += \"\\n**Missing Documents:**\\n\"\n",
        "                    for doc in analysis_summary.get('missing_documents', []):\n",
        "                        status_summary += f\"   - {doc}\\n\"\n",
        "\n",
        "                return status_summary, report_json\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                return \"✅ Analysis complete (parsing error)\", report_json\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"❌ Analysis failed: {str(e)}\"\n",
        "            return error_msg, json.dumps({\"error\": str(e)}, indent=2)\n",
        "\n",
        "        finally:\n",
        "            # Cleanup temp directory\n",
        "            try:\n",
        "                shutil.rmtree(temp_dir, ignore_errors=True)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    # Create Gradio interface\n",
        "    with gr.Blocks(\n",
        "        title=\"ADGM Corporate Agent\",\n",
        "        theme=gr.themes.Soft(),\n",
        "        css=\"\"\"\n",
        "        .gradio-container {\n",
        "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
        "        }\n",
        "        .title {\n",
        "            text-align: center;\n",
        "            background: linear-gradient(90deg, #1e3a8a 0%, #3b82f6 100%);\n",
        "            color: white;\n",
        "            padding: 20px;\n",
        "            margin-bottom: 20px;\n",
        "            border-radius: 10px;\n",
        "        }\n",
        "        .info-box {\n",
        "            background-color: #f0f9ff;\n",
        "            border: 1px solid #0284c7;\n",
        "            padding: 15px;\n",
        "            border-radius: 8px;\n",
        "            margin: 10px 0;\n",
        "        }\n",
        "        \"\"\"\n",
        "    ) as demo:\n",
        "\n",
        "        # Title and description\n",
        "        gr.HTML(\"\"\"\n",
        "        <div class=\"title\">\n",
        "            <h1>🏛️ ADGM Corporate Agent</h1>\n",
        "            <h3>AI-Powered Legal Document Intelligence & Compliance Checker</h3>\n",
        "            <p>Abu Dhabi Global Market (ADGM) Document Review System</p>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "        gr.HTML(\"\"\"\n",
        "        <div class=\"info-box\">\n",
        "            <h4>📋 What this system does:</h4>\n",
        "            <ul>\n",
        "                <li><strong>Document Analysis:</strong> Reviews DOCX/PDF files for ADGM compliance</li>\n",
        "                <li><strong>Process Detection:</strong> Automatically identifies legal processes (Incorporation, Employment, etc.)</li>\n",
        "                <li><strong>Compliance Checking:</strong> Validates documents against ADGM regulations</li>\n",
        "                <li><strong>Document Annotation:</strong> Adds review comments directly to DOCX files</li>\n",
        "                <li><strong>Completeness Verification:</strong> Checks if all required documents are present</li>\n",
        "                <li><strong>RAG-Enhanced Analysis:</strong> Uses ADGM reference documents for accurate guidance</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "        # File upload section\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                files_input = gr.File(\n",
        "                    file_count=\"multiple\",\n",
        "                    file_types=[\".docx\", \".pdf\"],\n",
        "                    label=\"📁 Upload Legal Documents\",\n",
        "\n",
        "                )\n",
        "\n",
        "                analyze_btn = gr.Button(\n",
        "                    \"🔍 Analyze Documents\",\n",
        "                    variant=\"primary\",\n",
        "                    size=\"lg\"\n",
        "                )\n",
        "\n",
        "        # Results section\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                status_output = gr.Markdown(\n",
        "                    label=\"📊 Analysis Summary\",\n",
        "                    value=\"Upload documents and click 'Analyze Documents' to begin...\"\n",
        "                )\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                report_output = gr.Code(\n",
        "                    label=\"📄 Detailed JSON Report\",\n",
        "                    language=\"json\",\n",
        "                    lines=30,\n",
        "                    value=\"Detailed analysis report will appear here...\"\n",
        "                )\n",
        "\n",
        "        # Footer information\n",
        "        gr.HTML(\"\"\"\n",
        "        <div class=\"info-box\">\n",
        "            <h4>📚 Supported Document Types:</h4>\n",
        "            <div style=\"display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 10px;\">\n",
        "                <div>\n",
        "                    <strong>Company Formation:</strong>\n",
        "                    <ul style=\"margin: 5px 0;\">\n",
        "                        <li>Articles of Association</li>\n",
        "                        <li>Memorandum of Association</li>\n",
        "                        <li>Board Resolutions</li>\n",
        "                        <li>Incorporation Forms</li>\n",
        "                    </ul>\n",
        "                </div>\n",
        "                <div>\n",
        "                    <strong>Employment & HR:</strong>\n",
        "                    <ul style=\"margin: 5px 0;\">\n",
        "                        <li>Employment Contracts</li>\n",
        "                        <li>HR Policies</li>\n",
        "                        <li>Disciplinary Procedures</li>\n",
        "                    </ul>\n",
        "                </div>\n",
        "                <div>\n",
        "                    <strong>Data Protection:</strong>\n",
        "                    <ul style=\"margin: 5px 0;\">\n",
        "                        <li>Appropriate Policy Documents</li>\n",
        "                        <li>Privacy Notices</li>\n",
        "                        <li>Data Processing Records</li>\n",
        "                    </ul>\n",
        "                </div>\n",
        "                <div>\n",
        "                    <strong>Branch Registration:</strong>\n",
        "                    <ul style=\"margin: 5px 0;\">\n",
        "                        <li>Parent Company Documents</li>\n",
        "                        <li>Financial Statements</li>\n",
        "                        <li>Registration Applications</li>\n",
        "                    </ul>\n",
        "                </div>\n",
        "            </div>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "        gr.HTML(\"\"\"\n",
        "        <div style=\"text-align: center; padding: 20px; color: #6b7280; font-size: 12px;\">\n",
        "            <p><strong>Disclaimer:</strong> This automated review is for guidance only. Professional legal advice should be sought for final compliance verification.</p>\n",
        "            <p>© 2024 ADGM Corporate Agent - Powered by AI & RAG Technology</p>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "        # Event handlers\n",
        "        analyze_btn.click(\n",
        "            fn=process_files_interface,\n",
        "            inputs=[files_input],\n",
        "            outputs=[status_output, report_output],\n",
        "            show_progress=True\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🚀 Starting ADGM Corporate Agent...\")\n",
        "    print(\"📚 Loading knowledge base and initializing system...\")\n",
        "\n",
        "    try:\n",
        "        # Create and launch Gradio interface\n",
        "        demo = create_gradio_interface()\n",
        "\n",
        "        # Launch with public sharing enabled\n",
        "        demo.launch(\n",
        "            share=True,\n",
        "            inbrowser=True,\n",
        "            server_name=\"0.0.0.0\",\n",
        "            server_port=7860,\n",
        "            show_error=True,\n",
        "            debug=True\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to start application: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ============================================================================\n",
        "# TESTING FUNCTIONS (Optional)\n",
        "# ============================================================================\n",
        "\n",
        "def test_system_with_sample_documents():\n",
        "    \"\"\"Test function to verify system functionality with sample documents\"\"\"\n",
        "    print(\"🧪 Running system tests...\")\n",
        "\n",
        "    # Initialize knowledge base\n",
        "    kb = ADGMKnowledgeBase()\n",
        "    kb.load_reference_documents()\n",
        "\n",
        "    # Test document type detection\n",
        "    sample_texts = {\n",
        "        \"employment\": \"This Employment Contract is made between the Employer and Employee...\",\n",
        "        \"articles\": \"Articles of Association of XYZ Company Limited registered in ADGM...\",\n",
        "        \"resolution\": \"Board Resolution for incorporation of company in Abu Dhabi Global Market...\"\n",
        "    }\n",
        "\n",
        "    print(\"\\n📝 Testing document type detection:\")\n",
        "    for doc_type, text in sample_texts.items():\n",
        "        detected = detect_document_type(text)\n",
        "        print(f\"   {doc_type}: detected as '{detected}' ✓\")\n",
        "\n",
        "    print(\"\\n🔍 Testing RAG search:\")\n",
        "    if kb.embeddings_model:\n",
        "        results = kb.search_relevant_content(\"employment contract requirements\", top_k=2)\n",
        "        print(f\"   Found {len(results)} relevant chunks ✓\")\n",
        "    else:\n",
        "        print(\"   RAG search using keyword fallback ✓\")\n",
        "\n",
        "    print(\"\\n✅ System test completed successfully!\")\n",
        "    return True\n",
        "\n",
        "# Uncomment to run tests\n",
        "# test_system_with_sample_documents()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9jdGrVC53ZNH",
        "outputId": "c02ed14c-a756-4fda-936e-bfa70c31d2cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting ADGM Corporate Agent...\n",
            "📚 Loading knowledge base and initializing system...\n",
            "Initializing ADGM Knowledge Base...\n",
            "✓ RAG embeddings model initialized\n",
            "Loading ADGM reference documents...\n",
            "✓ Loaded checklist_1: 0 chunks\n",
            "✓ Loaded checklist_2: 0 chunks\n",
            "✓ Loaded employment_2019: 40 chunks\n",
            "✓ Loaded employment_2024: 50 chunks\n",
            "✓ Loaded data_sources: 2 chunks\n",
            "✓ Loaded data_protection: 0 chunks\n",
            "✓ Loaded resolution_template: 9 chunks\n",
            "✓ Created FAISS index with 101 embeddings\n",
            "Knowledge base loaded with 101 chunks\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://02e4f2f966d1148fcb.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://02e4f2f966d1148fcb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting analysis of 1 documents...\n",
            "Processing: ADGM Standard Employment Contract - ER 2019 - Short Version May 2024.docx\n",
            "Detected type: employment_contract\n",
            "Error annotating document /tmp/adgm_analysis_4jqrg7go/ADGM Standard Employment Contract - ER 2019 - Short Version May 2024.docx: 'Paragraph' object has no attribute 'insert_paragraph_after'\n",
            "✓ Annotated: ADGM Standard Employment Contract - ER 2019 - Short Version May 2024.docx\n",
            "Detected process: Employment/HR\n",
            "Document completeness: 1/3\n",
            "Starting analysis of 1 documents...\n",
            "Processing: ADGM Standard Employment Contract Template - ER 2024 Feb 2025.docx\n",
            "Detected type: employment_contract\n",
            "Error annotating document /tmp/adgm_analysis_7_gkjmj2/ADGM Standard Employment Contract Template - ER 2024 Feb 2025.docx: 'Paragraph' object has no attribute 'insert_paragraph_after'\n",
            "✓ Annotated: ADGM Standard Employment Contract Template - ER 2024 Feb 2025.docx\n",
            "Detected process: Employment/HR\n",
            "Document completeness: 1/3\n",
            "Starting analysis of 1 documents...\n",
            "Processing: adgm-ra-resolution-multiple-incorporate-shareholders-LTD-incorporation-v2.docx\n",
            "Detected type: resolution\n",
            "Error annotating document /tmp/adgm_analysis_35imb37q/adgm-ra-resolution-multiple-incorporate-shareholders-LTD-incorporation-v2.docx: 'Paragraph' object has no attribute 'insert_paragraph_after'\n",
            "✓ Annotated: adgm-ra-resolution-multiple-incorporate-shareholders-LTD-incorporation-v2.docx\n",
            "Detected process: Company Incorporation\n",
            "Document completeness: 1/11\n",
            "Starting analysis of 2 documents...\n",
            "Processing: adgm-ra-resolution-multiple-incorporate-shareholders-LTD-incorporation-v2.docx\n",
            "Detected type: resolution\n",
            "Error annotating document /tmp/adgm_analysis_7ejfmyz_/adgm-ra-resolution-multiple-incorporate-shareholders-LTD-incorporation-v2.docx: 'Paragraph' object has no attribute 'insert_paragraph_after'\n",
            "✓ Annotated: adgm-ra-resolution-multiple-incorporate-shareholders-LTD-incorporation-v2.docx\n",
            "Processing: Data Sources.docx\n",
            "Detected type: resolution\n",
            "Error annotating document /tmp/adgm_analysis_7ejfmyz_/Data Sources.docx: 'Paragraph' object has no attribute 'insert_paragraph_after'\n",
            "✓ Annotated: Data Sources.docx\n",
            "Detected process: Company Incorporation\n",
            "Document completeness: 1/11\n",
            "Starting analysis of 1 documents...\n",
            "Processing: Data Sources.docx\n",
            "Detected type: resolution\n",
            "Error annotating document /tmp/adgm_analysis_am9g451o/Data Sources.docx: 'Paragraph' object has no attribute 'insert_paragraph_after'\n",
            "✓ Annotated: Data Sources.docx\n",
            "Detected process: Company Incorporation\n",
            "Document completeness: 0/11\n",
            "Starting analysis of 1 documents...\n",
            "Processing: ADGM Standard Employment Contract Template - ER 2024 Feb 2025.docx\n",
            "Detected type: employment_contract\n",
            "Error annotating document /tmp/adgm_analysis_2ls7bq6r/ADGM Standard Employment Contract Template - ER 2024 Feb 2025.docx: 'Paragraph' object has no attribute 'insert_paragraph_after'\n",
            "✓ Annotated: ADGM Standard Employment Contract Template - ER 2024 Feb 2025.docx\n",
            "Detected process: Employment/HR\n",
            "Document completeness: 1/3\n",
            "Starting analysis of 1 documents...\n",
            "Processing: adgm-ra-resolution-multiple-incorporate-shareholders-LTD-incorporation-v2.docx\n",
            "Detected type: resolution\n",
            "Error annotating document /tmp/adgm_analysis_5qsqq9f4/adgm-ra-resolution-multiple-incorporate-shareholders-LTD-incorporation-v2.docx: 'Paragraph' object has no attribute 'insert_paragraph_after'\n",
            "✓ Annotated: adgm-ra-resolution-multiple-incorporate-shareholders-LTD-incorporation-v2.docx\n",
            "Detected process: Company Incorporation\n",
            "Document completeness: 1/11\n"
          ]
        }
      ]
    }
  ]
}